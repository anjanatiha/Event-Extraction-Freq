{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a842ccb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from operator import itemgetter\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import words\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import framenet as fn\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus.reader.framenet import PrettyList\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from time import time\n",
    "# ...\n",
    "# nltk.download('words')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4583b792",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_summary_main = r\"data/BBC News Summary/News Articles/\"\n",
    "sub_dirs = os.listdir(directory_summary_main)\n",
    "\n",
    "word_count_dict = {}\n",
    "root_word_count_dict = {}\n",
    "\n",
    "total_token_count = 0\n",
    "max_file_count = 1000\n",
    "\n",
    "i = 0\n",
    "\n",
    "for sub in sub_dirs:\n",
    "    files = os.listdir(directory_summary_main + sub)\n",
    "    for f in files:\n",
    "        try:\n",
    "            text = open(directory_summary_main + sub + \"/\" + f, \"r\").read()\n",
    "            tokens = word_tokenize(text.lower())\n",
    "            total_token_count = total_token_count + len(tokens)\n",
    "            for tok in tokens:\n",
    "                if tok.isalpha():\n",
    "                    if tok not in word_count_dict:\n",
    "                        word_count_dict[tok] = 0\n",
    "                    word_count_dict[tok] = word_count_dict[tok] + 1\n",
    "            i = i + 1\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        #     if i % 1000==1:\n",
    "        #         print(i)\n",
    "        #     if i > max_file_count:\n",
    "        #         break\n",
    "\n",
    "word_freq_dict = {}\n",
    "for w in word_count_dict:\n",
    "    word_freq_dict[w] = word_count_dict[w]/total_token_count\n",
    "\n",
    "word_freq_dict_val = {k: v for k, v in sorted(word_freq_dict.items(), key=lambda x: x[1], reverse= False)}  \n",
    "\n",
    "with open(\"word_fequency.json\", \"w\") as outfile:\n",
    "    json.dump(word_freq_dict_val, outfile)\n",
    "    \n",
    "# word_freq_dict_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0f11ea4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_event_typing(w, thres = 0.0, m = 3, depth = 3):\n",
    "    ws = wn.synsets(w)\n",
    "    \n",
    "    hyper = lambda s: s.hypernyms()\n",
    "    types1 = {}\n",
    "    types2 = {}\n",
    "\n",
    "    for e in ws:\n",
    "        w_tmp = e.name().split(\".\")[0]\n",
    "        if w_tmp==w:\n",
    "            t = 0\n",
    "            c = 0 \n",
    "            i = 0\n",
    "            tmp_types = \"\"\n",
    "            \n",
    "            tmp_event_types = list(e.closure(hyper))\n",
    "            \n",
    "            for tmp_event in tmp_event_types:\n",
    "                d = wn.path_similarity(e, tmp_event)\n",
    "                tmp_event_name = tmp_event.name().split(\".\")[0]\n",
    "                \n",
    "                b = m**i\n",
    "                t = t + (d/b)\n",
    "                types2[tmp_event_name] = d\n",
    "                \n",
    "                if d and (d >= (thres/t)):\n",
    "                    tmp_types = tmp_types + \":\" + tmp_event_name\n",
    "                    if c >= depth - 1:\n",
    "                        break\n",
    "                    c = c + 1\n",
    "                    \n",
    "                i = i + 1\n",
    "                \n",
    "            if len(tmp_types) > 0:\n",
    "                types1[tmp_types] = t\n",
    "                \n",
    "    types1 = {k: v for k, v in sorted(types1.items(), key=lambda item: item[1], reverse=True)}\n",
    "    types2 = {k: v for k, v in sorted(types2.items(), key=lambda item: item[1], reverse=True)}\n",
    "    \n",
    "    return types1, types2\n",
    "\n",
    "\n",
    "def get_event_typing_main(w, pos, thres = 0.0, m = 3, depth = 3):\n",
    "    if not pos:\n",
    "        pos = nltk.tag.pos_tag(w.split())[0][1]\n",
    "    \n",
    "    types1, types2 = get_event_typing(w, thres, m, depth)\n",
    "    \n",
    "    if not (types1 or types2):\n",
    "        if pos.startswith(\"V\"):    \n",
    "            w_tmp = WordNetLemmatizer().lemmatize(w, \"v\")\n",
    "            types1, types2 = get_event_typing(w_tmp, thres, m, depth)\n",
    "            if (types1 or types2):\n",
    "                return types1, types2\n",
    "            \n",
    "        if pos.startswith(\"N\"):\n",
    "            w_tmp = WordNetLemmatizer().lemmatize(w, \"n\")\n",
    "            types1, types2 = get_event_typing(w_tmp, thres, m, depth)\n",
    "            if (types1 or types2):\n",
    "                return types1, types2\n",
    "            \n",
    "        w_tmp = WordNetLemmatizer().lemmatize(w)\n",
    "        types1, types2 = get_event_typing(w_tmp, thres, m, depth)\n",
    "        \n",
    "        if (types1 or types2):\n",
    "            return types1, types2\n",
    "        \n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "        w_tmp = stemmer.stem(w)\n",
    "        types1, types2 = get_event_typing(w_tmp, thres, m, depth)\n",
    "            \n",
    "    \n",
    "    return types1, types2\n",
    "\n",
    "\n",
    "def replacer(match):\n",
    "    if match.group(1) is not None:\n",
    "        return '{} '.format(match.group(1))\n",
    "    else:\n",
    "        return ' {}'.format(match.group(2))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    rx = re.compile(r'^(\\W+)|(\\W+)$')\n",
    "    text = \" \".join([rx.sub(replacer, word) for word in raw_text.split()])\n",
    "    text = re.sub(\"'\", \" ' \", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "408f192f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vaccine NN\n",
      "candidate NN\n",
      "study NN\n",
      "evaluate VB\n",
      "vaccine NN\n",
      "safety NN\n",
      "dose NN\n"
     ]
    }
   ],
   "source": [
    "stops = set(stopwords.words('english'))\n",
    "upper_freq_noun = 0.0001\n",
    "upper_freq_verb = 0.00001\n",
    "\n",
    "\n",
    "\n",
    "raw_text = \"Pfizer and BioNTech have begun a clinical trial for their Omicron-specific Covid-19 vaccine candidate, they announced in a news release on Tuesday. The study will evaluate the vaccine for safety, tolerability and the level of immune response, as both a primary series and a booster dose, in up to 1420 healthy adults ages 18 to 55.\"\n",
    "text = preprocess_text(raw_text)\n",
    "\n",
    "# print(text)\n",
    "tokens = text.lower().split()\n",
    "\n",
    "pos_tags = nltk.tag.pos_tag(text.split())\n",
    "for x in pos_tags:\n",
    "    token = x[0].lower()\n",
    "    pos = x[1]\n",
    "    if (token not in stops) and ((pos.startswith(\"V\")) or (pos==\"NN\")) and (token in word_freq_dict_val):\n",
    "        if (pos.startswith(\"V\")) and (word_freq_dict_val[token] > upper_freq_verb):\n",
    "            continue\n",
    "        if (pos==\"NN\") and (word_freq_dict_val[token] > upper_freq_noun):\n",
    "            continue\n",
    "        a, b = get_event_typing_main(token, pos=pos, thres=0.0, m=10, depth=3) \n",
    "        if a or b:\n",
    "#             print(\"_\"*100)\n",
    "#             print(token, end=\", \")\n",
    "            print(token, pos)\n",
    "#         if a:\n",
    "#             print(a)\n",
    "#             print(\"\\n\")\n",
    "#         if b:\n",
    "#             print(b)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "47575b5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.163847391357053e-06"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq_dict_val[\"ash\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
