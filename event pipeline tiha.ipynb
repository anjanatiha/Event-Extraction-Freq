{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4a54d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from operator import itemgetter\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import words\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import framenet as fn\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus.reader.framenet import PrettyList\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from time import time\n",
    "# ...\n",
    "# nltk.download('words')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eeff8ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_summary_main = r\"data/BBC News Summary/News Articles/\"\n",
    "sub_dirs = os.listdir(directory_summary_main)\n",
    "\n",
    "word_count_dict = {}\n",
    "root_word_count_dict = {}\n",
    "\n",
    "total_token_count = 0\n",
    "max_file_count = 1000\n",
    "\n",
    "i = 0\n",
    "\n",
    "for sub in sub_dirs:\n",
    "    files = os.listdir(directory_summary_main + sub)\n",
    "    for f in files:\n",
    "        try:\n",
    "            text = open(directory_summary_main + sub + \"/\" + f, \"r\").read()\n",
    "            tokens = word_tokenize(text.lower())\n",
    "            total_token_count = total_token_count + len(tokens)\n",
    "            for tok in tokens:\n",
    "                if tok.isalpha():\n",
    "                    if tok not in word_count_dict:\n",
    "                        word_count_dict[tok] = 0\n",
    "                    word_count_dict[tok] = word_count_dict[tok] + 1\n",
    "            i = i + 1\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        #     if i % 1000==1:\n",
    "        #         print(i)\n",
    "        #     if i > max_file_count:\n",
    "        #         break\n",
    "\n",
    "word_freq_dict = {}\n",
    "for w in word_count_dict:\n",
    "    word_freq_dict[w] = word_count_dict[w]/total_token_count\n",
    "\n",
    "word_freq_dict_val = {k: v for k, v in sorted(word_freq_dict.items(), key=lambda x: x[1], reverse= False)}  \n",
    "\n",
    "with open(\"word_fequency.json\", \"w\") as outfile:\n",
    "    json.dump(word_freq_dict_val, outfile)\n",
    "    \n",
    "# word_freq_dict_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f11ea4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_event_typing(w, thres = 0.0, m = 3, depth = 3):\n",
    "    ws = wn.synsets(w)\n",
    "    \n",
    "    hyper = lambda s: s.hypernyms()\n",
    "    types1 = {}\n",
    "    types2 = {}\n",
    "\n",
    "    for e in ws:\n",
    "        w_tmp = e.name().split(\".\")[0]\n",
    "        if w_tmp==w:\n",
    "            t = 0\n",
    "            c = 0 \n",
    "            i = 0\n",
    "            tmp_types = \"\"\n",
    "            \n",
    "            tmp_event_types = list(e.closure(hyper))\n",
    "            \n",
    "            for tmp_event in tmp_event_types:\n",
    "                d = wn.path_similarity(e, tmp_event)\n",
    "                tmp_event_name = tmp_event.name().split(\".\")[0]\n",
    "                \n",
    "                b = m**i\n",
    "                t = t + (d/b)\n",
    "                types2[tmp_event_name] = d\n",
    "                \n",
    "                if d and (d >= (thres/t)):\n",
    "                    tmp_types = tmp_types + \":\" + tmp_event_name\n",
    "                    if c >= depth - 1:\n",
    "                        break\n",
    "                    c = c + 1\n",
    "                    \n",
    "                i = i + 1\n",
    "                \n",
    "            if len(tmp_types) > 0:\n",
    "                types1[tmp_types] = t\n",
    "                \n",
    "    types1 = {k: v for k, v in sorted(types1.items(), key=lambda item: item[1], reverse=True)}\n",
    "    types2 = {k: v for k, v in sorted(types2.items(), key=lambda item: item[1], reverse=True)}\n",
    "    \n",
    "    return types1, types2\n",
    "\n",
    "\n",
    "def get_event_typing_main(w, pos, thres = 0.0, m = 3, depth = 3):\n",
    "    if not pos:\n",
    "        pos = nltk.tag.pos_tag(w.split())[0][1]\n",
    "    \n",
    "    types1, types2 = get_event_typing(w, thres, m, depth)\n",
    "    \n",
    "    if not (types1 or types2):\n",
    "        if pos.startswith(\"V\"):    \n",
    "            w_tmp = WordNetLemmatizer().lemmatize(w, \"v\")\n",
    "            types1, types2 = get_event_typing(w_tmp, thres, m, depth)\n",
    "            if (types1 or types2):\n",
    "                return types1, types2\n",
    "            \n",
    "        if pos.startswith(\"N\"):\n",
    "            w_tmp = WordNetLemmatizer().lemmatize(w, \"n\")\n",
    "            types1, types2 = get_event_typing(w_tmp, thres, m, depth)\n",
    "            if (types1 or types2):\n",
    "                return types1, types2\n",
    "            \n",
    "        w_tmp = WordNetLemmatizer().lemmatize(w)\n",
    "        types1, types2 = get_event_typing(w_tmp, thres, m, depth)\n",
    "        \n",
    "        if (types1 or types2):\n",
    "            return types1, types2\n",
    "        \n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "        w_tmp = stemmer.stem(w)\n",
    "        types1, types2 = get_event_typing(w_tmp, thres, m, depth)\n",
    "            \n",
    "    \n",
    "    return types1, types2\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be6a1f2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stops = set(stopwords.words('english'))\n",
    "upper_freq_noun = 0.00001\n",
    "upper_freq_verb = 0.00001\n",
    "\n",
    "text = \"A firefighter and his crew battled to keep the raging Glass Fire from devastating an upmarket Napa Valley vineyard. The firefighter denies lighting backfires which consume fuel in a wildfire's path but admits his team failed to advise Cal Fire, the state's fire agency that it was in the evacuated area, as required by law.\"\n",
    "tokens = text.lower().split()\n",
    "\n",
    "pos_tags = nltk.tag.pos_tag(text.split())\n",
    "for x in pos_tags:\n",
    "    token = x[0].lower()\n",
    "    pos = x[1]\n",
    "    if (token not in stops) and (pos.startswith(\"V\") or pos==\"NN\"):\n",
    "        if (pos.startswith(\"V\")) and (token in word_freq_dict_val) and (word_freq_dict_val[token] > upper_freq_verb):\n",
    "            continue\n",
    "        if (pos==\"NN\") and (token in word_freq_dict_val) and (word_freq_dict_val[token] > upper_freq_noun):\n",
    "            continue\n",
    "#         print(token)\n",
    "        a, b = get_event_typing_main(token, pos=pos, thres=0.0, m=10, depth=3) \n",
    "        print(token, pos)\n",
    "#         if a:\n",
    "#             print(a)\n",
    "#             print(\"\\n\")\n",
    "#         if b:\n",
    "#             print(b)\n",
    "#         print(\"_\"*100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
