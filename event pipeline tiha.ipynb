{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "25e67634",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import words\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import framenet as fn\n",
    "from time import time\n",
    "from nltk.corpus import stopwords\n",
    "from operator import itemgetter\n",
    "from nltk.corpus.reader.framenet import PrettyList\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "# ...\n",
    "# nltk.download('words')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa66c501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# event = {\"calamity\": {\"famine\", \"plague\", \"tidal_wave\", \"tsunami\" , \"Avalanche\", \"Blizzard\", \"Earthquake\", \"Fire\", \"Flood\", \"Heat wave\", \"Hurricane\", \"Landslide\", \"Lightning strike\", \"Volcanic eruption\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0f11ea4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{':symptom:evidence:information': 0.5358333333333333, ':action:state:attribute': 0.5358333333333333, ':emergence:beginning:happening': 0.5358333333333333}\n",
      "{'symptom': 0.5, 'action': 0.5, 'emergence': 0.5, 'evidence': 0.3333333333333333, 'state': 0.3333333333333333, 'beginning': 0.3333333333333333, 'information': 0.25, 'attribute': 0.25, 'happening': 0.25}\n"
     ]
    }
   ],
   "source": [
    "def get_event_typing(w, thres = 0.0, m = 3, depth = 3):\n",
    "    hyper = lambda s: s.hypernyms()\n",
    "    ws = wn.synsets(w)\n",
    "    types1 = {}\n",
    "    types2 = {}\n",
    "    for e in ws:\n",
    "        w_tmp = e.name().split(\".\")[0]\n",
    "        if w_tmp==w:\n",
    "            tmp_event_types = list(e.closure(hyper))\n",
    "            t = 0\n",
    "            c = 0 \n",
    "            i = 0\n",
    "            tmp_types = \"\"\n",
    "            for tmp_event in tmp_event_types:\n",
    "                d = wn.path_similarity(e, tmp_event)\n",
    "                tmp_event_name = tmp_event.name().split(\".\")[0]\n",
    "                b = m**i\n",
    "                t = t + d/b\n",
    "                types2[tmp_event_name] = d\n",
    "                if d and (d>=thres/t):\n",
    "                    tmp_types = tmp_types + \":\" + tmp_event_name\n",
    "                    c = c + 1\n",
    "                    if c >= depth:\n",
    "                        break\n",
    "                i = i + 1\n",
    "            if len(tmp_types) > 0:\n",
    "                types1[tmp_types] = t\n",
    "                \n",
    "    types1 = {k: v for k, v in sorted(types1.items(), key=lambda item: item[1], reverse=True)}\n",
    "    types2 = {k: v for k, v in sorted(types2.items(), key=lambda item: item[1], reverse=True)}\n",
    "    \n",
    "    return types1, types2\n",
    "\n",
    "\n",
    "def get_event_typing_main(w, pos, thres = 0.0, m = 3, depth = 3):\n",
    "    if not pos:\n",
    "        pos = nltk.tag.pos_tag(w.split())[0][1]\n",
    "    types1, types2 = get_event_typing(w, thres, m, depth)\n",
    "    if not (types1 or types2):\n",
    "        if pos.startswith(\"V\"):    \n",
    "            w_tmp = WordNetLemmatizer().lemmatize(w, \"v\")\n",
    "            types1, types2 = get_event_typing(w_tmp, thres, m, depth)\n",
    "            if (types1 or types2):\n",
    "                return types1, types2\n",
    "        if pos.startswith(\"N\"):\n",
    "            w_tmp = WordNetLemmatizer().lemmatize(w, \"n\")\n",
    "            types1, types2 = get_event_typing(w_tmp, thres, m, depth)\n",
    "            if (types1 or types2):\n",
    "                return types1, types2\n",
    "            \n",
    "        w_tmp = WordNetLemmatizer().lemmatize(w)\n",
    "        types1, types2 = get_event_typing(w_tmp, thres, m, depth)\n",
    "        if (types1 or types2):\n",
    "            return types1, types2\n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "        w_tmp = stemmer.stem(w)\n",
    "        types1, types2 = get_event_typing(w_tmp, thres, m, depth)\n",
    "            \n",
    "    \n",
    "    return types1, types2\n",
    "            \n",
    "            \n",
    "text = \"eruption\"\n",
    "\n",
    "a, b = get_event_typing_main(text, pos=\"\", thres=0.0, m=10, depth=3) \n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "76729f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "general\n",
      "bomb\n"
     ]
    }
   ],
   "source": [
    "WordNetLemmatizer().lemmatize(\"bombed\")\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "print(stemmer.stem(\"generalized\"))\n",
    "print(stemmer.stem(\"bombed\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d0411683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "s = wn.synsets('walks')\n",
    "\n",
    "for i in s:\n",
    "#     print(i.definition())\n",
    "#     x = fn.frames_by_lemma('(?i)' + i.definition() + '(?i)')\n",
    "    l = i.name().split(\".\")[0]\n",
    "#     print(l)\n",
    "    a = fn.frames_by_lemma(\"rise_up\")\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe17e27f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f68b5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = fn.frames(r'(?i)crim')\n",
    "x.sort(key=itemgetter('ID'))\n",
    "[i.name for i in x]\n",
    "# len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f9dc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn.frames_by_lemma(r'(?i)'+\"walk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c88a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wf = json.load(open(\"word_fequency.json\"))\n",
    "wfo = {}\n",
    "c = 0\n",
    "for t in wf:\n",
    "    c = c + wf[t]\n",
    "\n",
    "for t in wf:\n",
    "    wfo[t] = wf[t]/c\n",
    "    \n",
    "    \n",
    "json_object = json.dumps(wfo)\n",
    "\n",
    "with open(\"word_fequency_norm.json\", \"w\") as outfile:\n",
    "    outfile.write(json_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa41a1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''New York City Police Department Chief of Detectives James Essig detailed what suspect Frank James did after shooting at passengers on a Brooklyn subway on Tuesday.\n",
    "\n",
    "\"We believe, but this is still early in the investigation, that after firing his weapon 33 times at innocent New York City subway riders, Mr. James boarded an R train that had pulled into the station, went one stop up and exited at the 25th Street station. We also have a picture of that. The gun used in this — a 9 millimeter Glock — which was recovered at this crime scene, was purchased by Mr. James in 2011 in Ohio,\" Essig said.\n",
    "The official also noted that the construction helmet James was wearing was recovered in a garbage bin.\n",
    "\n",
    "Essig said officials then tracked James' location before he was arrested.\n",
    "\n",
    "\"We tracked Mr. James, and his last known whereabouts was 7th Avenue and 9th Street in Park Slope, entering the subway. Minutes ago, thankfully, NYPD patrol officers from the 9th Precinct responded to St. Marks [Place] and First Avenue, where they apprehended him without incident,\" Essig said, noting that the suspect was in Park Slope at about 9:15 a.m. ET yesterday.\n",
    "\n",
    "Kenneth Corey, the NYPD chief of department, described how the arrest unfolded today, explaining that investigators got a CrimeStoppers tip that the suspect was in a McDonald's at 6th Street and First Avenue in the East Village.\n",
    "\n",
    "\"Officers respond to the McDonald's. He's not in the McDonald's. They start driving around the neighborhood looking for him, they see him on the corner of St. Marks [Place] and First [Avenue] and they take him into custody,\" he said.\n",
    "\n",
    "The case was quickly solved using technology and video canvassing, he added. He also thanked federal and regional partners.\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3c0cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigger_type_prediction = 'None'\n",
    "\n",
    "            x = fn.frames('(?i)' + tokens[tmp_trigger_position[0]] + '(?i)')\n",
    "            x.sort(key=itemgetter('ID'))\n",
    "            if len(x) > 0:\n",
    "                trigger_type_prediction = x[0][\"name\"]\n",
    "            # print(\"Token: \", tokens[tmp_trigger_position[0]], \" , trigger_type_prediction:\", trigger_type_prediction)\n",
    "        \n",
    "            else:\n",
    "                x = fn.frames('(?i)' + WordNetLemmatizer().lemmatize(tokens[tmp_trigger_position[0]],'v') + '(?i)')\n",
    "                # x.sort(key=itemgetter('ID'))\n",
    "                if len(x) > 0:\n",
    "                    trigger_type_prediction = x[0][\"name\"]\n",
    "                else:\n",
    "                    event_types_dict = get_event_typing(tokens[tmp_trigger_position[0]], thres = 0.0, m = 3, depth=3)\n",
    "                    if len(event_types_dict)>0:\n",
    "                        trigger_type_prediction = [elem for elem in event_types_dict.values()][0]\n",
    "                    elif len(trigger_type_prediction)==0:\n",
    "                        if tmp_trigger_position in predicate_sense:\n",
    "                            x = fn.frames_by_lemma('(?i)' + predicate_sense[tmp_trigger_position] + '(?i)')\n",
    "                            # x.sort(key=itemgetter('ID'))\n",
    "                            if len(x) > 0:\n",
    "                                trigger_type_prediction = x[0][\"name\"]\n",
    "                            else:\n",
    "                                trigger_type_prediction = predicate_sense[tmp_trigger_position]\n",
    "                        elif tmp_trigger_position in detected_mentions:\n",
    "                            trigger_type_prediction = detected_mentions[tmp_trigger_position]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b577a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_event_typing(w, thres = 0.0, m = 3, depth = 3):\n",
    "    x = fn.frames('(?i)' + tokens[tmp_trigger_position[0]] + '(?i)')\n",
    "    \n",
    "    pos = nltk.tag.pos_tag(w.split())[0]\n",
    "    print(pos)\n",
    "    if len(pos)==0:\n",
    "        return []\n",
    "    w = pos[0]\n",
    "    ps = pos[1]\n",
    "    \n",
    "        \n",
    "    \n",
    "    print(ps)\n",
    "    \n",
    "    hyper = lambda s: s.hypernyms()\n",
    "    ws = wn.synsets(w)\n",
    "    print(ws)\n",
    "    types = {}\n",
    "    for e in ws:\n",
    "        if e.name().split(\".\")[0]==w:\n",
    "            tmp_event_types = list(e.closure(hyper))\n",
    "            t = 0\n",
    "            c = 0 \n",
    "            i = 0\n",
    "            tmp_types = \"\"\n",
    "            for tmp_event in tmp_event_types:\n",
    "                d = wn.path_similarity(e, tmp_event)\n",
    "        #         print(tmp_event, d)\n",
    "                b = m**i\n",
    "                t = t + d/b\n",
    "                if d and (d>=thres/b):\n",
    "                    tmp_types = tmp_types + \":\" + tmp_event.name().split(\".\")[0]\n",
    "                    c = c + 1\n",
    "                    if c >= depth:\n",
    "                        break\n",
    "                i = i + 1\n",
    "            if len(tmp_types) > 0:\n",
    "                types[t] = tmp_types\n",
    "        #     print(\"\\n\")\n",
    "\n",
    "\n",
    "    types = {k: v for k, v in sorted(types.items(), key=lambda item: item[0], reverse=True)}\n",
    "    \n",
    "    return types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fa4a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_event_typing(\"slept\", thres = 0.0, m = 3, depth = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0e7de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "filtered_words = set(stopwords.words('english'))\n",
    "\n",
    "wfo = json.load(open(\"word_fequency_NORM.json\"))\n",
    "\n",
    "# text = \"The eruption of an underwater volcano near Tonga on Saturday was likely the biggest recorded anywhere on the planet in more than 30 years, according to experts. Dramatic images from space captured the eruption in real time, as a huge plume of ash, gas and steam was spewed up to 20 kilometers into the atmosphere and tsunami waves were sent crashing across the Pacific.\"\n",
    "# text = \"At least seven historically Black colleges and universities (HBCUs) across the United States received back-to-back bomb threats this week, forcing students to evacuate or shelter in place while authorities investigated. The threats come amid a dramatic rise in bombings in the US and follow bomb threats at other US colleges last November.\"\n",
    "# text = \"Crews fighting a massive fire along the central coast of California near the iconic Highway 1 made progress Sunday in containing the blaze, but dozens of homes remained under evacuation orders. The Colorado fire ignited Friday evening in Palo Colorado Canyon in the Big Sur region of Monterey County and swelled to 1050 acres Saturday, up from 100 acres a day prior.\"\n",
    "\n",
    "text = re.sub('\\n+',' ', text)\n",
    "text = re.sub('\\t+',' ', text)\n",
    "text = re.sub('\\s+',' ', text)\n",
    "\n",
    "number_of_sentences = len(sent_tokenize(text))\n",
    "\n",
    "\n",
    "# print(text)\n",
    "print(\"Number of Sentences: \", number_of_sentences)\n",
    "print(\"Number of Tokens: \", len(word_tokenize(text)))\n",
    "print(\"Number of Characters: \", len(text))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "verb_max = 0.00001\n",
    "noun_max = 0.00005\n",
    "\n",
    "start_time = time()\n",
    "pos = nltk.tag.pos_tag(text.split())\n",
    "\n",
    "\n",
    "for p in pos:\n",
    "    event_types = []\n",
    "    w = p[0].lower()\n",
    "    ps = p[1]\n",
    "#     print(p[0], ps)\n",
    "    if ps ==\"NNS\" or ps ==\"NNP\":\n",
    "        continue\n",
    "    if ((not ps==\"NN\") and (not ps.startswith(\"VB\"))) or ((not (p[0] in words.words())) or (not (p[0] in wf))):\n",
    "        tmp_w = WordNetLemmatizer().lemmatize(w,'v')\n",
    "        if tmp_w and (tmp_w is not w):\n",
    "            w = tmp_w\n",
    "            ps = nltk.tag.pos_tag(w.split())[0][1]\n",
    "        else:\n",
    "            tmp_w = WordNetLemmatizer().lemmatize(w,'n')\n",
    "            if tmp_w and (tmp_w is not w):\n",
    "                w = tmp_w\n",
    "                ps = nltk.tag.pos_tag(w.split())[0][1]\n",
    "    if ps==\"NN\" or ps.startswith(\"VB\"):\n",
    "        if (not w in filtered_words) and (w in words.words()) and (w in wfo):\n",
    "            if ps==\"NN\" and wfo[w] > noun_max:\n",
    "                continue\n",
    "            if ps.startswith(\"VB\") and wfo[w] > verb_max:\n",
    "                continue\n",
    "            if p[0].lower() in wfo:\n",
    "                if ps==\"NN\" and wfo[p[0].lower()] > noun_max:\n",
    "                    continue\n",
    "                if ps.startswith(\"VB\") and wfo[p[0].lower()] > verb_max:\n",
    "                    continue\n",
    "            event_types_dict = get_event_typing(w, thres = 0.0, m = 3, depth=3)\n",
    "            event_types = [elem for elem in event_types_dict.values()]\n",
    "            if len(event_types)==0:\n",
    "                print(\"f\")\n",
    "                f = fn.frames(p[0].lower())\n",
    "                if len(f) > 0:\n",
    "                    event_types = [t[\"name\"] for t in f]\n",
    "                else:\n",
    "                    f = fn.frames_by_lemma(p[0].lower())\n",
    "                    if len(f) > 0:\n",
    "                        event_types = [t[\"name\"] for t in f]\n",
    "                    else:\n",
    "                        f = fn.frames(w)\n",
    "                        if len(f) > 0:\n",
    "                            event_types = [t[\"name\"] for t in f]\n",
    "                        else:\n",
    "                            f = fn.frames_by_lemma(w)\n",
    "                            if len(f) > 0:\n",
    "                                event_types = [t[\"name\"] for t in f]\n",
    "            print(p[0], event_types, wfo[w], ps, w)\n",
    "#             print(p[0])\n",
    "#             print(w)\n",
    "\n",
    "\n",
    "total_time = time() - start_time\n",
    "avg_time = total_time/number_of_sentences\n",
    "print(\"\\n\")\n",
    "print(\"Total Time: \", total_time)\n",
    "print(\"Average Time per Sentence: \", avg_time)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64617b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "suspect \n",
    "shooting - \n",
    "passanger \n",
    "subway\n",
    "investigation \n",
    "firing - \n",
    "weapon \n",
    "crime \n",
    "purchased\n",
    "tracked -\n",
    "suspect \n",
    "arrest \n",
    "suspect\n",
    "case \n",
    "solved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232710b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "a =\"reported believed regarding  has disputed was according yesterday confidence version events missile strike warship source intelligence\"\n",
    "\n",
    "pos = nltk.tag.pos_tag(\"yesterday\".split())\n",
    "pos"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
