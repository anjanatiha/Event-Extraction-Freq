{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2c644ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from operator import itemgetter\n",
    "import nltk\n",
    "# !pip install prettyList\n",
    "# import prettyList\n",
    "# nltk.download('framenet_v17')\n",
    "from nltk.corpus import framenet as fn\n",
    "from nltk.corpus.reader.framenet import PrettyList\n",
    "x = fn.frames_by_lemma(r'(?i)reading book')\n",
    "# x = fn.frames(r'crime')\n",
    "# x.sort(key=itemgetter('ID'))\n",
    "print(x)\n",
    "for i in x:\n",
    "    print(i[\"name\"])\n",
    "len(x)\n",
    "# sorted(x, key=itemgetter('ID'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9f059db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "press\n",
      "[<frame ID=1622 name=Expressing_publicly>, <frame ID=161 name=Facial_expression>, ...]\n",
      "Give_impression\n",
      "Facial_expression\n",
      "Expressing_publicly\n",
      "Impression\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "w = WordNetLemmatizer().lemmatize(\"pressing\", 'v')\n",
    "print(w)\n",
    "x = fn.frames(w)\n",
    "print(x)\n",
    "x.sort(key=itemgetter('ID'))\n",
    "\n",
    "for i in x:\n",
    "    print(i[\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "9572a99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('bombing.n.01'), Synset('bombing.n.02'), Synset('bombard.v.02'), Synset('fail.v.07')]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'hypO' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-266-15a678dc7223>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhypO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'hypO' is not defined"
     ]
    }
   ],
   "source": [
    "hyper = lambda s: s.hypernyms()\n",
    "\n",
    "s = [\"bombing\", \"volcano\", \"storm\", \"flood\"]\n",
    "a = wn.synsets(s[0])\n",
    "print(a)\n",
    "b = a[1]\n",
    "list(b.closure(hypO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "b75e90e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "event = {\"calamity\": {\"famine\", \"plague\", \"tidal_wave\", \"tsunami\" , \"Avalanche\", \"Blizzard\", \"Earthquake\", \"Fire\", \"Flood\", \"Heat wave\", \"Hurricane\", \"Landslide\", \"Lightning strike\", \"Volcanic eruption\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "0f11ea4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_event_typing(w, thres = 0.0, m = 3, depth = 3):\n",
    "    hyper = lambda s: s.hypernyms()\n",
    "    ws = wn.synsets(w)\n",
    "    types = {}\n",
    "    for e in ws:\n",
    "        tmp_event_types = list(e.closure(hyper))\n",
    "        t = 0\n",
    "        c = 0 \n",
    "        i = 0\n",
    "        tmp_types = \"\"\n",
    "        for tmp_event in tmp_event_types:\n",
    "            d = wn.path_similarity(e, tmp_event)\n",
    "    #         print(tmp_event, d)\n",
    "            b = m**i\n",
    "            t = t + d/b\n",
    "            if d and (d>=thres/b):\n",
    "                tmp_types = tmp_types + \":\" + tmp_event.name().split(\".\")[0]\n",
    "                c = c + 1\n",
    "                if c >= depth:\n",
    "                    break\n",
    "            i = i + 1\n",
    "        if len(tmp_types) > 0:\n",
    "            types[t] = tmp_types\n",
    "    #     print(\"\\n\")\n",
    "\n",
    "\n",
    "    types = {k: v for k, v in sorted(types.items(), key=lambda item: item[0], reverse=True)}\n",
    "    \n",
    "    return types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "5840832a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "wf = json.load(open(\"word_fequency.json\"))\n",
    "wfo = {}\n",
    "c = 0\n",
    "for t in wf:\n",
    "    c = c + wf[t]\n",
    "\n",
    "for t in wf:\n",
    "    wfo[t] = wf[t]/c\n",
    "    \n",
    "    \n",
    "json_object = json.dumps(wfo)\n",
    "\n",
    "with open(\"word_fequency_norm.json\", \"w\") as outfile:\n",
    "    outfile.write(json_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "6c3655c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crews NN 1.3316772111289475e-05\n",
      "fighting VBG 6.90050918494091e-05\n",
      "fire NN 3.9950316333868424e-05\n",
      "coast NN 2.4212312929617228e-05\n",
      "containing NN 2.0580465990174642e-05\n",
      "dozens NN 1.57380034042512e-05\n",
      "evacuation NN 1.2106156464808614e-06\n",
      "fire NN 3.9950316333868424e-05\n",
      "ignited NN 4.842462585923445e-06\n",
      "evening NN 7.263693878885168e-05\n",
      "region NN 7.021570749588996e-05\n",
      "swelled VBD 1.2106156464808614e-06\n",
      "acres NN 3.6318469394425843e-06\n",
      "acres NN 3.6318469394425843e-06\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import words\n",
    "\n",
    "# nltk.download('words')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "wfo = json.load(open(\"word_fequency_NORM.json\"))\n",
    "\n",
    "text = \"The eruption of an underwater volcano near Tonga on Saturday was likely the biggest recorded anywhere on the planet in more than 30 years, according to experts. Dramatic images from space captured the eruption in real time, as a huge plume of ash, gas and steam was spewed up to 20 kilometers into the atmosphere and tsunami waves were sent crashing across the Pacific.\"\n",
    "# text = \"At least seven historically Black colleges and universities (HBCUs) across the United States received back-to-back bomb threats this week, forcing students to evacuate or shelter in place while authorities investigated. The threats come amid a dramatic rise in bombings in the US and follow bomb threats at other US colleges last November.\"\n",
    "text = \"Crews fighting a massive fire along the central coast of California near the iconic Highway 1 made progress Sunday in containing the blaze, but dozens of homes remained under evacuation orders. The Colorado fire ignited Friday evening in Palo Colorado Canyon in the Big Sur region of Monterey County and swelled to 1050 acres Saturday, up from 100 acres a day prior.\"\n",
    "\n",
    "c = 0.0001\n",
    "\n",
    "\n",
    "toks = text.lower().split()\n",
    "\n",
    "pos = nltk.tag.pos_tag(text.split())\n",
    "\n",
    "for p in pos:\n",
    "    w = p[0].lower()\n",
    "    ps = p[1]\n",
    "    if ps ==\"NNS\":\n",
    "        con\n",
    "    if ((not ps==\"NN\") and (not ps.startswith(\"VB\"))) or ((not (p[0] in words.words())) or (not (p[0] in wf))):\n",
    "        tmp_w = WordNetLemmatizer().lemmatize(w,'v')\n",
    "        if tmp_w and (tmp_w is not w):\n",
    "            w = tmp_w\n",
    "            ps = nltk.tag.pos_tag(w.split())[0][1]\n",
    "        else:\n",
    "            tmp_w = WordNetLemmatizer().lemmatize(w,'n')\n",
    "            if tmp_w and (tmp_w is not w):\n",
    "                w = tmp_w\n",
    "                ps = nltk.tag.pos_tag(w.split())[0][1]\n",
    "    if ps==\"NN\" or ps.startswith(\"VB\"):\n",
    "        if w in words.words() and w in wf:\n",
    "            if wfo[w] < c:\n",
    "                event_types_dict = get_event_typing(w, thres = 0.0, m = 3, depth=3)\n",
    "                event_types = [elem for elem in event_types_dict.values()]\n",
    "                print(p[0], ps, wfo[w])\n",
    "#                 print(w, event_types)\n",
    "    else:\n",
    "        pass\n",
    "#         print(w)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "c04eb9a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NN'"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WordNetLemmatizer().lemmatize(\"threats\",'v')\n",
    "nltk.tag.pos_tag(\"acre\".split())[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0563b771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hurricane.n.01  vs.  disaster.n.01\n",
      "--------------------------------------------------\n",
      "0.05263157894736842\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'hypernyms'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-75e1d43d2eb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhypernyms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhypernyms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlowest_common_hypernyms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'hypernyms'"
     ]
    }
   ],
   "source": [
    "a = 'hurricane.n.01'\n",
    "b = 'disaster.n.01'\n",
    "\n",
    "c = wn.synset(a)\n",
    "d = wn.synset(b)\n",
    "\n",
    "print(a.split()[0], \" vs. \", b.split()[0])\n",
    "print(\"-\"*50)\n",
    "print(wn.path_similarity(c, d))\n",
    "# print(a.hypernyms(), a.hypernyms())\n",
    "print(wn.synset(a).lowest_common_hypernyms(wn.synset(b)))\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "a = 'cat.n.01'\n",
    "b = 'disaster.n.01'\n",
    "\n",
    "c = wn.synset(a)\n",
    "d = wn.synset(b)\n",
    "\n",
    "print(a.split()[0], \" vs. \", b.split()[0])\n",
    "print(\"-\"*50)\n",
    "print(wn.path_similarity(c, d))\n",
    "print(wn.synset(a).lowest_common_hypernyms(wn.synset(b)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
