{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6072d686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# event = {\"calamity\": {\"famine\", \"plague\", \"tidal_wave\", \"tsunami\" , \"Avalanche\", \"Blizzard\", \"Earthquake\", \"Fire\", \"Flood\", \"Heat wave\", \"Hurricane\", \"Landslide\", \"Lightning strike\", \"Volcanic eruption\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f11ea4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_event_typing(w, thres = 0.0, m = 3, depth = 3):\n",
    "    hyper = lambda s: s.hypernyms()\n",
    "    ws = wn.synsets(w)\n",
    "    types = {}\n",
    "    for e in ws:\n",
    "        tmp_event_types = list(e.closure(hyper))\n",
    "        t = 0\n",
    "        c = 0 \n",
    "        i = 0\n",
    "        tmp_types = \"\"\n",
    "        for tmp_event in tmp_event_types:\n",
    "            d = wn.path_similarity(e, tmp_event)\n",
    "    #         print(tmp_event, d)\n",
    "            b = m**i\n",
    "            t = t + d/b\n",
    "            if d and (d>=thres/b):\n",
    "                tmp_types = tmp_types + \":\" + tmp_event.name().split(\".\")[0]\n",
    "                c = c + 1\n",
    "                if c >= depth:\n",
    "                    break\n",
    "            i = i + 1\n",
    "        if len(tmp_types) > 0:\n",
    "            types[t] = tmp_types\n",
    "    #     print(\"\\n\")\n",
    "\n",
    "\n",
    "    types = {k: v for k, v in sorted(types.items(), key=lambda item: item[0], reverse=True)}\n",
    "    \n",
    "    return types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b33071d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "wf = json.load(open(\"word_fequency.json\"))\n",
    "wfo = {}\n",
    "c = 0\n",
    "for t in wf:\n",
    "    c = c + wf[t]\n",
    "\n",
    "for t in wf:\n",
    "    wfo[t] = wf[t]/c\n",
    "    \n",
    "    \n",
    "json_object = json.dumps(wfo)\n",
    "\n",
    "with open(\"word_fequency_norm.json\", \"w\") as outfile:\n",
    "    outfile.write(json_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2135124",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''Ukrainian President Volodymyr Zelensky announced Tuesday on Telegram that Viktor Medvedchuk, a pro-Russian Ukrainian politician and oligarch, had been detained in a \"special operation.\"\n",
    "\n",
    "Zelensky posted a photo of a handcuffed and disheveled-looking Medvedchuk wearing fatigues, with the caption, \"A special operation was carried out thanks to the SBU [the Security Service of Ukraine]. Well done! Details later.\"\n",
    "Prior to Russia's invasion of Ukraine, Medvedchuk had faced allegations of treason in Ukraine and had been under house arrest. His whereabouts had been unknown in the weeks following the invasion. Some observers speculated that Medvedchuk or one of his allies might be the Kremlin's preference to lead a puppet government in Ukraine if the Feb. 24 invasion succeeded in toppling Zelensky.\n",
    "\n",
    "Medvedchuk was sanctioned by the US in 2014 \"for threatening the peace, security, stability, sovereignty, or territorial integrity of Ukraine, and for undermining Ukraine’s democratic institutions and processes.\"\n",
    "\n",
    "But the wealthy businessman also served as a go-between for Moscow and Kyiv after the outbreak of the Donbas conflict in 2014 by leveraging his personal ties with Putin. In a 2019 interview with filmmaker Oliver Stone, Putin acknowledged that he was godfather to Medvedchuk's daughter.\n",
    "\n",
    "\"I would not say that we are very close but we know each other well,\" Putin said. \"He was [former Ukrainian] President [Leonid] Kuchma’s chief of staff, and it was in this capacity at the time that he asked me to take part in the christening of his daughter. According to Russian Orthodox tradition, you can't refuse such a request.\"\n",
    "\n",
    "Medvedchuk also had notoriety in Ukraine for his role as the Soviet state-appointed defense attorney for the Ukrainian dissident poet Vasyl Stus, who died in a Soviet labor camp in 1985.\n",
    "\n",
    "In a statement, SBU head Ivan Bakanov said, \"You may be a pro-Russian politician and work for the aggressor state for years. You may hide from justice lately. You may even wear a Ukrainian military uniform for camouflage… But will it help you to escape punishment? Not at all! Shackles are waiting for you. And for the same traitors of Ukraine as you!\"\n",
    "\n",
    "Bakanov added, \"Pro-Russian traitors and agents of the Russian intelligence services, remember — your crimes have no statute of limitations. And there are no hiding places where we wouldn’t find you!\"\n",
    "\n",
    "CNN was not immediately able to reach a legal representative for Medvedchuk.\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0feae6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Sentences:  2\n",
      "Number of Tokens:  69\n",
      "Number of Characters:  369\n",
      "\n",
      "\n",
      "eruption [':emergence:beginning:happening'] 1.2106156464808614e-06 NN eruption\n",
      "volcano [':mountain:natural_elevation:geological_formation'] 1.2106156464808614e-06 NN volcano\n",
      "recorded [':indicate:inform:communicate', ':remind'] 0.0004951417994106723 NN record\n",
      "planet [':celestial_body:natural_object:whole'] 1.4527387757770337e-05 NN planet\n",
      "according [':give:transfer'] 0.0005108798028149235 VBG according\n",
      "space [':location:space:object', ':type:block:artifact', ':put:move'] 8.111124831421772e-05 NN space\n",
      "captured [':represent:re-create:make', ':assume:take', ':get'] 1.4527387757770337e-05 NN capture\n",
      "eruption [':emergence:beginning:happening'] 1.2106156464808614e-06 NN eruption\n",
      "gas [':fuel:hydrocarbon:substance', ':overstate:misinform:inform', ':attack:contend'] 8.353247960717944e-05 NN gas\n",
      "steam [':cook:change_integrity:change', ':clean:change', ':emit'] 1.815923469721292e-05 NN steam\n",
      "atmosphere [':quality:attribute:abstraction'] 3.75290850409067e-05 NN atmosphere\n",
      "tsunami [':calamity:wave:misfortune'] 0.00014043141499177992 NN tsunami\n",
      "sent [':publicize:tell:inform', ':transfer:move', ':move'] 0.00018401357826509094 VBN sent\n",
      "crashing [':hurl:throw:propel', ':bed_down:go_to_bed', ':change'] 3.631846939442584e-05 NN crash\n",
      "\n",
      "\n",
      "Total Time:  2.9618098735809326\n",
      "Average Time per Sentence:  1.4809049367904663\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import words\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import framenet as fn\n",
    "from time import time\n",
    "# nltk.download('words')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('punkt')\n",
    "\n",
    "\n",
    "wfo = json.load(open(\"word_fequency_NORM.json\"))\n",
    "\n",
    "text = \"The eruption of an underwater volcano near Tonga on Saturday was likely the biggest recorded anywhere on the planet in more than 30 years, according to experts. Dramatic images from space captured the eruption in real time, as a huge plume of ash, gas and steam was spewed up to 20 kilometers into the atmosphere and tsunami waves were sent crashing across the Pacific.\"\n",
    "# text = \"At least seven historically Black colleges and universities (HBCUs) across the United States received back-to-back bomb threats this week, forcing students to evacuate or shelter in place while authorities investigated. The threats come amid a dramatic rise in bombings in the US and follow bomb threats at other US colleges last November.\"\n",
    "# text = \"Crews fighting a massive fire along the central coast of California near the iconic Highway 1 made progress Sunday in containing the blaze, but dozens of homes remained under evacuation orders. The Colorado fire ignited Friday evening in Palo Colorado Canyon in the Big Sur region of Monterey County and swelled to 1050 acres Saturday, up from 100 acres a day prior.\"\n",
    "\n",
    "text = re.sub('\\n+',' ', text)\n",
    "text = re.sub('\\t+',' ', text)\n",
    "text = re.sub('\\s+',' ', text)\n",
    "\n",
    "number_of_sentences = len(sent_tokenize(text))\n",
    "\n",
    "\n",
    "# print(text)\n",
    "print(\"Number of Sentences: \", number_of_sentences)\n",
    "print(\"Number of Tokens: \", len(word_tokenize(text)))\n",
    "print(\"Number of Characters: \", len(text))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "verb_max = 0.001\n",
    "noun_max = 0.001\n",
    "\n",
    "start_time = time()\n",
    "pos = nltk.tag.pos_tag(text.split())\n",
    "\n",
    "\n",
    "for p in pos:\n",
    "    event_types = []\n",
    "    w = p[0].lower()\n",
    "    ps = p[1]\n",
    "    if ps ==\"NNS\":\n",
    "        continue\n",
    "    if ((not ps==\"NN\") and (not ps.startswith(\"VB\"))) or ((not (p[0] in words.words())) or (not (p[0] in wf))):\n",
    "        tmp_w = WordNetLemmatizer().lemmatize(w,'v')\n",
    "        if tmp_w and (tmp_w is not w):\n",
    "            w = tmp_w\n",
    "            ps = nltk.tag.pos_tag(w.split())[0][1]\n",
    "        else:\n",
    "            tmp_w = WordNetLemmatizer().lemmatize(w,'n')\n",
    "            if tmp_w and (tmp_w is not w):\n",
    "                w = tmp_w\n",
    "                ps = nltk.tag.pos_tag(w.split())[0][1]\n",
    "    if ps==\"NN\" or ps.startswith(\"VB\"):\n",
    "        if w in words.words() and w in wfo:\n",
    "            if ps==\"NN\" and wfo[w] > noun_max:\n",
    "                continue\n",
    "            if ps.startswith(\"VB\") and wfo[w] > verb_max:\n",
    "                continue\n",
    "            if p[0].lower() in wfo:\n",
    "                if ps==\"NN\" and wfo[p[0].lower()] > noun_max:\n",
    "                    continue\n",
    "                if ps.startswith(\"VB\") and wfo[p[0].lower()] > verb_max:\n",
    "                    continue\n",
    "            event_types_dict = get_event_typing(w, thres = 0.0, m = 3, depth=3)\n",
    "            event_types = [elem for elem in event_types_dict.values()]\n",
    "            if len(event_types)==0:\n",
    "                f = fn.frames(p[0].lower())\n",
    "                if len(f) > 0:\n",
    "                    event_types = [t[\"name\"] for t in f]\n",
    "                else:\n",
    "                    f = fn.frames_by_lemma(p[0].lower())\n",
    "                    if len(f) > 0:\n",
    "                        event_types = [t[\"name\"] for t in f]\n",
    "                    else:\n",
    "                        f = fn.frames(w)\n",
    "                        if len(f) > 0:\n",
    "                            event_types = [t[\"name\"] for t in f]\n",
    "                        else:\n",
    "                            f = fn.frames_by_lemma(w)\n",
    "                            if len(f) > 0:\n",
    "                                event_types = [t[\"name\"] for t in f]\n",
    "            print(p[0], event_types, wfo[w], ps, w)\n",
    "#                 print(p[0])\n",
    "#                 print(w)\n",
    "\n",
    "\n",
    "total_time = time() - start_time\n",
    "avg_time = total_time/number_of_sentences\n",
    "print(\"\\n\")\n",
    "print(\"Total Time: \", total_time)\n",
    "print(\"Average Time per Sentence: \", avg_time)\n",
    "                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
